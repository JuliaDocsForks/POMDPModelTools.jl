{
    "docs": [
        {
            "location": "/", 
            "text": "About\n\n\nPOMDPModelTools is a collection of interface extensions and tools to make writing models and solvers for \nPOMDPs.jl\n easier.\n\n\n\n\nModel Transformations\n\n\nFully Observable POMDP\n\n\nGenerative Belief MDP\n\n\nUnderlying MDP\n\n\n\n\n\n\nConvenience\n\n\nDistributions\n\n\nSparse Categorical (\nSparseCat\n)\n\n\nBool Distribution\n\n\nDeterministic\n\n\n\n\n\n\nInterface Extensions\n\n\nWeighted Iteration\n\n\nObservation Weight\n\n\nOrdered Spaces\n\n\nInfo Interface\n\n\n\n\n\n\nAbout\n\n\nUtility Types\n\n\nTerminal State", 
            "title": "Home"
        }, 
        {
            "location": "/#about", 
            "text": "POMDPModelTools is a collection of interface extensions and tools to make writing models and solvers for  POMDPs.jl  easier.   Model Transformations  Fully Observable POMDP  Generative Belief MDP  Underlying MDP    Convenience  Distributions  Sparse Categorical ( SparseCat )  Bool Distribution  Deterministic    Interface Extensions  Weighted Iteration  Observation Weight  Ordered Spaces  Info Interface    About  Utility Types  Terminal State", 
            "title": "About"
        }, 
        {
            "location": "/convenience/", 
            "text": "Convenience\n\n\nPOMDPModelTools contains default implementations for some POMDPs.jl functions, including \nactions\n, \nn_actions\n, \naction_index\n, etc. for some obvious cases. This allows some obvious implementations to be skipped.\n\n\nFor instance, if an MDP with a Bool action type is created, the obvious \nactions\n method is already implemented:\n\n\njulia\n using POMDPs; using POMDPModelTools\n\njulia\n struct BoolMDP \n: MDP{Bool, Bool} end\n\njulia\n actions(BoolMDP())\n(true, false)\n\n\n\n\nFor a complete list of default implementations, see \nconvenient_implementations.jl\n and \ndistributions_jl.jl\n.", 
            "title": "Convenience"
        }, 
        {
            "location": "/convenience/#convenience", 
            "text": "POMDPModelTools contains default implementations for some POMDPs.jl functions, including  actions ,  n_actions ,  action_index , etc. for some obvious cases. This allows some obvious implementations to be skipped.  For instance, if an MDP with a Bool action type is created, the obvious  actions  method is already implemented:  julia  using POMDPs; using POMDPModelTools\n\njulia  struct BoolMDP  : MDP{Bool, Bool} end\n\njulia  actions(BoolMDP())\n(true, false)  For a complete list of default implementations, see  convenient_implementations.jl  and  distributions_jl.jl .", 
            "title": "Convenience"
        }, 
        {
            "location": "/distributions/", 
            "text": "Distributions\n\n\nPOMDPModelTools contains several utility distributions to be used in the POMDPs \ntransition\n and \nobservation\n functions. These implement the appropriate methods of the functions in the \ndistributions interface\n.\n\n\n\n\nSparse Categorical (\nSparseCat\n)\n\n\nSparseCat\n is a sparse categorical distribution which is specified by simply providing a list of possible values (states or observations) and the probabilities corresponding to those particular objects.\n\n\nExample: \nSparseCat([1,2,3], [0.1,0.2,0.7])\n is a categorical distribution that assignes probability 0.1 to \n1\n, 0.2 to \n2\n, 0.7 to \n3\n, and 0 to all other values.\n\n\n#\n\n\nPOMDPModelTools.SparseCat\n \n \nType\n.\n\n\nSparseCat(values, probabilities)\n\n\n\n\nCreate a sparse categorical distribution.\n\n\nvalues\n is an iterable object containing the possible values (can be of any type) in the distribution that have nonzero probability. \nprobabilities\n is an iterable object that contains the associated probabilities.\n\n\nThis is optimized for value iteration with a fast implementation of \nweighted_iterator\n. Both \npdf\n and \nrand\n are order n.\n\n\nsource\n\n\n\n\nBool Distribution\n\n\n#\n\n\nPOMDPModelTools.BoolDistribution\n \n \nType\n.\n\n\nBoolDistribution(p_true)\n\n\n\n\nCreate a distribution over Boolean values (\ntrue\n or \nfalse\n).\n\n\np_true\n is the probability of the \ntrue\n outcome; the probability of \nfalse\n is 1-\np_true\n.\n\n\nsource\n\n\n\n\nDeterministic\n\n\n#\n\n\nPOMDPModelTools.Deterministic\n \n \nType\n.\n\n\nDetrministic(value)\n\n\n\n\nCreate a deterministic distribution over only one value.\n\n\nThis is intended to be used when a distribution is required, but the outcome is deterministic. It is equivalent to a Kronecker Delta distribution.\n\n\nsource", 
            "title": "Distributions"
        }, 
        {
            "location": "/distributions/#distributions", 
            "text": "POMDPModelTools contains several utility distributions to be used in the POMDPs  transition  and  observation  functions. These implement the appropriate methods of the functions in the  distributions interface .", 
            "title": "Distributions"
        }, 
        {
            "location": "/distributions/#sparse-categorical-sparsecat", 
            "text": "SparseCat  is a sparse categorical distribution which is specified by simply providing a list of possible values (states or observations) and the probabilities corresponding to those particular objects.  Example:  SparseCat([1,2,3], [0.1,0.2,0.7])  is a categorical distribution that assignes probability 0.1 to  1 , 0.2 to  2 , 0.7 to  3 , and 0 to all other values.  #  POMDPModelTools.SparseCat     Type .  SparseCat(values, probabilities)  Create a sparse categorical distribution.  values  is an iterable object containing the possible values (can be of any type) in the distribution that have nonzero probability.  probabilities  is an iterable object that contains the associated probabilities.  This is optimized for value iteration with a fast implementation of  weighted_iterator . Both  pdf  and  rand  are order n.  source", 
            "title": "Sparse Categorical (SparseCat)"
        }, 
        {
            "location": "/distributions/#bool-distribution", 
            "text": "#  POMDPModelTools.BoolDistribution     Type .  BoolDistribution(p_true)  Create a distribution over Boolean values ( true  or  false ).  p_true  is the probability of the  true  outcome; the probability of  false  is 1- p_true .  source", 
            "title": "Bool Distribution"
        }, 
        {
            "location": "/distributions/#deterministic", 
            "text": "#  POMDPModelTools.Deterministic     Type .  Detrministic(value)  Create a deterministic distribution over only one value.  This is intended to be used when a distribution is required, but the outcome is deterministic. It is equivalent to a Kronecker Delta distribution.  source", 
            "title": "Deterministic"
        }, 
        {
            "location": "/interface_extensions/", 
            "text": "Interface Extensions\n\n\nPOMDPModelTools contains several interface extensions that provide shortcuts and standardized ways of dealing with extra data.\n\n\nProgrammers should use these functions whenever possible in case optimized implementations are available, but all of the functions have default implementations based on the core POMDPs.jl interface. Thus, if the core interface is implemented, all of these functions will also be available.\n\n\n\n\nWeighted Iteration\n\n\nMany solution techniques, for example value iteration, require iteration through the support of a distribution and evaluating the probability mass for each value. In some cases, looking up the probability mass is expensive, so it is more efficient to iterate through value =\n probability pairs. \nweighted_iterator\n provides a standard interface for this.\n\n\n#\n\n\nPOMDPModelTools.weighted_iterator\n \n \nFunction\n.\n\n\nweighted_iterator(d)\n\n\n\n\nReturn an iterator through pairs of the values and probabilities in distribution \nd\n.\n\n\nThis is designed to speed up value iteration. Distributions are encouraged to provide a custom optimized implementation if possible.\n\n\nExample\n\n\njulia\n d = BoolDistribution(0.7)\nBoolDistribution(0.7)\n\njulia\n collect(weighted_iterator(d))\n2-element Array{Pair{Bool,Float64},1}:\n  true =\n 0.7\n false =\n 0.3\n\n\n\n\nsource\n\n\n\n\nObservation Weight\n\n\nSometimes, e.g. in particle filtering, the relative likelihood of an observation is required in addition to a generative model, and it is often tedious to implement a custom observation distribution type. For this case, the shortcut function \nobs_weight\n is provided.\n\n\n#\n\n\nPOMDPModelTools.obs_weight\n \n \nFunction\n.\n\n\nobs_weight(pomdp, sp, o)\nobs_weight(pomdp, a, sp, o)\nobs_weight(pomdp, s, a, sp, o)\n\n\n\n\nReturn a weight proportional to the likelihood of receiving observation o from state sp (and a and s if they are present).\n\n\nThis is a useful shortcut for particle filtering so that the observation distribution does not have to be represented.\n\n\nsource\n\n\n\n\nOrdered Spaces\n\n\nIt is often useful to have a list of states, actions, or observations ordered consistently with the respective \n_index\n function from POMDPs.jl. Since the POMDPs.jl interface does not demand that spaces be ordered consistently with \n_index\n, the \nstates\n, \nactions\n, and \nobservations\n functions are not sufficient. Thus POMDPModelTools provides \nordered_actions\n, \nordered_states\n, and \nordered_observations\n to provide this capability.\n\n\n#\n\n\nPOMDPModelTools.ordered_actions\n \n \nFunction\n.\n\n\nordered_actions(mdp)\n\n\n\n\nReturn an \nAbstractVector\n of actions ordered according to \nactionindex(mdp, a)\n.\n\n\nordered_actions(mdp)\n will always return an \nAbstractVector{A}\n \nv\n containing all of the actions in \nactions(mdp)\n in the order such that \nactionindex(mdp, v[i]) == i\n. You may wish to override this for your problem for efficiency.\n\n\nsource\n\n\n#\n\n\nPOMDPModelTools.ordered_states\n \n \nFunction\n.\n\n\nordered_states(mdp)\n\n\n\n\nReturn an \nAbstractVector\n of states ordered according to \nstateindex(mdp, a)\n.\n\n\nordered_states(mdp)\n will always return a \nAbstractVector{A}\n \nv\n containing all of the states in \nstates(mdp)\n in the order such that \nstateindex(mdp, v[i]) == i\n. You may wish to override this for your problem for efficiency.\n\n\nsource\n\n\n#\n\n\nPOMDPModelTools.ordered_observations\n \n \nFunction\n.\n\n\nordered_observations(pomdp)\n\n\n\n\nReturn an \nAbstractVector\n of observations ordered according to \nobsindex(pomdp, a)\n.\n\n\nordered_observations(mdp)\n will always return a \nAbstractVector{A}\n \nv\n containing all of the observations in \nobservations(pomdp)\n in the order such that \nobsindex(pomdp, v[i]) == i\n. You may wish to override this for your problem for efficiency.\n\n\nsource\n\n\n\n\nInfo Interface\n\n\nIt is often the case that useful information besides the belief, state, action, etc is generated by a function in POMDPs.jl. This information can be useful for debugging or understanding the behavior of a solver, updater, or problem. The info interface provides a standard way for problems, policies, solvers or updaters to output this information. The recording simulators from \nPOMDPSimulators.jl\n automatically record this information.\n\n\n#\n\n\nPOMDPModelTools.generate_sri\n \n \nFunction\n.\n\n\nReturn a tuple containing the next state and reward and information (usually a \nDict\n or \nnothing\n) from that step.\n\n\nBy default, returns \nnothing\n as info.\n\n\nsource\n\n\n#\n\n\nPOMDPModelTools.generate_sori\n \n \nFunction\n.\n\n\nReturn a tuple containing the next state, observation, and reward and information (usually a \nDict\n or \nnothing\n) from that step.\n\n\nBy default, returns \nnothing\n as info. \n\n\nsource\n\n\n#\n\n\nPOMDPModelTools.action_info\n \n \nFunction\n.\n\n\na, ai = action_info(policy, x)\n\n\n\n\nReturn a tuple containing the action determined by policy 'p' at state or belief 'x' and information (usually a \nDict\n or \nnothing\n) from the calculation of that action.\n\n\nBy default, returns \nnothing\n as info.\n\n\nsource\n\n\n#\n\n\nPOMDPModelTools.solve_info\n \n \nFunction\n.\n\n\npolicy, si = solve_info(solver, problem)\n\n\n\n\nReturn a tuple containing the policy determined by a solver and information (usually a \nDict\n or \nnothing\n) from the calculation of that policy.\n\n\nBy default, returns \nnothing\n as info.\n\n\nsource\n\n\n#\n\n\nPOMDPModelTools.update_info\n \n \nFunction\n.\n\n\nbp, i = update_info(updater, b, a, o)\n\n\n\n\nReturn a tuple containing the new belief and information (usually a \nDict\n or \nnothing\n) from the belief update.\n\n\nBy default, returns \nnothing\n as info.\n\n\nsource", 
            "title": "Interface extensions"
        }, 
        {
            "location": "/interface_extensions/#interface-extensions", 
            "text": "POMDPModelTools contains several interface extensions that provide shortcuts and standardized ways of dealing with extra data.  Programmers should use these functions whenever possible in case optimized implementations are available, but all of the functions have default implementations based on the core POMDPs.jl interface. Thus, if the core interface is implemented, all of these functions will also be available.", 
            "title": "Interface Extensions"
        }, 
        {
            "location": "/interface_extensions/#weighted-iteration", 
            "text": "Many solution techniques, for example value iteration, require iteration through the support of a distribution and evaluating the probability mass for each value. In some cases, looking up the probability mass is expensive, so it is more efficient to iterate through value =  probability pairs.  weighted_iterator  provides a standard interface for this.  #  POMDPModelTools.weighted_iterator     Function .  weighted_iterator(d)  Return an iterator through pairs of the values and probabilities in distribution  d .  This is designed to speed up value iteration. Distributions are encouraged to provide a custom optimized implementation if possible.  Example  julia  d = BoolDistribution(0.7)\nBoolDistribution(0.7)\n\njulia  collect(weighted_iterator(d))\n2-element Array{Pair{Bool,Float64},1}:\n  true =  0.7\n false =  0.3  source", 
            "title": "Weighted Iteration"
        }, 
        {
            "location": "/interface_extensions/#observation-weight", 
            "text": "Sometimes, e.g. in particle filtering, the relative likelihood of an observation is required in addition to a generative model, and it is often tedious to implement a custom observation distribution type. For this case, the shortcut function  obs_weight  is provided.  #  POMDPModelTools.obs_weight     Function .  obs_weight(pomdp, sp, o)\nobs_weight(pomdp, a, sp, o)\nobs_weight(pomdp, s, a, sp, o)  Return a weight proportional to the likelihood of receiving observation o from state sp (and a and s if they are present).  This is a useful shortcut for particle filtering so that the observation distribution does not have to be represented.  source", 
            "title": "Observation Weight"
        }, 
        {
            "location": "/interface_extensions/#ordered-spaces", 
            "text": "It is often useful to have a list of states, actions, or observations ordered consistently with the respective  _index  function from POMDPs.jl. Since the POMDPs.jl interface does not demand that spaces be ordered consistently with  _index , the  states ,  actions , and  observations  functions are not sufficient. Thus POMDPModelTools provides  ordered_actions ,  ordered_states , and  ordered_observations  to provide this capability.  #  POMDPModelTools.ordered_actions     Function .  ordered_actions(mdp)  Return an  AbstractVector  of actions ordered according to  actionindex(mdp, a) .  ordered_actions(mdp)  will always return an  AbstractVector{A}   v  containing all of the actions in  actions(mdp)  in the order such that  actionindex(mdp, v[i]) == i . You may wish to override this for your problem for efficiency.  source  #  POMDPModelTools.ordered_states     Function .  ordered_states(mdp)  Return an  AbstractVector  of states ordered according to  stateindex(mdp, a) .  ordered_states(mdp)  will always return a  AbstractVector{A}   v  containing all of the states in  states(mdp)  in the order such that  stateindex(mdp, v[i]) == i . You may wish to override this for your problem for efficiency.  source  #  POMDPModelTools.ordered_observations     Function .  ordered_observations(pomdp)  Return an  AbstractVector  of observations ordered according to  obsindex(pomdp, a) .  ordered_observations(mdp)  will always return a  AbstractVector{A}   v  containing all of the observations in  observations(pomdp)  in the order such that  obsindex(pomdp, v[i]) == i . You may wish to override this for your problem for efficiency.  source", 
            "title": "Ordered Spaces"
        }, 
        {
            "location": "/interface_extensions/#info-interface", 
            "text": "It is often the case that useful information besides the belief, state, action, etc is generated by a function in POMDPs.jl. This information can be useful for debugging or understanding the behavior of a solver, updater, or problem. The info interface provides a standard way for problems, policies, solvers or updaters to output this information. The recording simulators from  POMDPSimulators.jl  automatically record this information.  #  POMDPModelTools.generate_sri     Function .  Return a tuple containing the next state and reward and information (usually a  Dict  or  nothing ) from that step.  By default, returns  nothing  as info.  source  #  POMDPModelTools.generate_sori     Function .  Return a tuple containing the next state, observation, and reward and information (usually a  Dict  or  nothing ) from that step.  By default, returns  nothing  as info.   source  #  POMDPModelTools.action_info     Function .  a, ai = action_info(policy, x)  Return a tuple containing the action determined by policy 'p' at state or belief 'x' and information (usually a  Dict  or  nothing ) from the calculation of that action.  By default, returns  nothing  as info.  source  #  POMDPModelTools.solve_info     Function .  policy, si = solve_info(solver, problem)  Return a tuple containing the policy determined by a solver and information (usually a  Dict  or  nothing ) from the calculation of that policy.  By default, returns  nothing  as info.  source  #  POMDPModelTools.update_info     Function .  bp, i = update_info(updater, b, a, o)  Return a tuple containing the new belief and information (usually a  Dict  or  nothing ) from the belief update.  By default, returns  nothing  as info.  source", 
            "title": "Info Interface"
        }, 
        {
            "location": "/model_transformations/", 
            "text": "Model Transformations\n\n\nPOMDPModelTools contains several tools for transforming problems into other classes so that they can be used by different solvers.\n\n\n\n\nFully Observable POMDP\n\n\n#\n\n\nPOMDPModelTools.FullyObservablePOMDP\n \n \nType\n.\n\n\nFullyObservablePOMDP(mdp)\n\n\n\n\nTurn \nMDP\n \nmdp\n into a \nPOMDP\n where the observations are the states of the MDP.\n\n\nsource\n\n\n\n\nGenerative Belief MDP\n\n\nEvery POMDP is an MDP on the belief space \nGenerativeBeliefMDP\n creates a generative model for that MDP.\n\n\n WARNING: The reward generated by the `GenerativeBeliefMDP` is the reward for a *single state sampled from the belief*; it is not the expected reward for that belief transition (though, in expectation, they are equivalent of course). Implementing the model with the expected reward requires a custom implementation because belief updaters do not typically deal with reward. \n\n\n\n#\n\n\nPOMDPModelTools.GenerativeBeliefMDP\n \n \nType\n.\n\n\nGenerativeBeliefMDP(pomdp, updater)\n\n\n\n\nCreate a generative model of the belief MDP corresponding to POMDP \npomdp\n with belief updates performed by \nupdater\n.\n\n\nsource\n\n\n\n\nExample\n\n\nusing POMDPModels\nusing POMDPModelTools\nusing BeliefUpdaters\n\npomdp = BabyPOMDP()\nupdater = DiscreteUpdater(pomdp)\n\nbelief_mdp = GenerativeBeliefMDP(pomdp, updater)\n@show statetype(belief_mdp) # POMDPModels.BoolDistribution\n\nfor (a, r, sp) in stepthrough(belief_mdp, RandomPolicy(belief_mdp), \na,r,sp\n, max_steps=5)\n    @show a, r, sp\nend\n\n\n\n\n\n\nUnderlying MDP\n\n\n#\n\n\nPOMDPModelTools.UnderlyingMDP\n \n \nType\n.\n\n\nUnderlyingMDP(pomdp)\n\n\n\n\nTransform \nPOMDP\n \npomdp\n into an \nMDP\n where the states are fully observed.\n\n\nsource", 
            "title": "Model transformations"
        }, 
        {
            "location": "/model_transformations/#model-transformations", 
            "text": "POMDPModelTools contains several tools for transforming problems into other classes so that they can be used by different solvers.", 
            "title": "Model Transformations"
        }, 
        {
            "location": "/model_transformations/#fully-observable-pomdp", 
            "text": "#  POMDPModelTools.FullyObservablePOMDP     Type .  FullyObservablePOMDP(mdp)  Turn  MDP   mdp  into a  POMDP  where the observations are the states of the MDP.  source", 
            "title": "Fully Observable POMDP"
        }, 
        {
            "location": "/model_transformations/#generative-belief-mdp", 
            "text": "Every POMDP is an MDP on the belief space  GenerativeBeliefMDP  creates a generative model for that MDP.   WARNING: The reward generated by the `GenerativeBeliefMDP` is the reward for a *single state sampled from the belief*; it is not the expected reward for that belief transition (though, in expectation, they are equivalent of course). Implementing the model with the expected reward requires a custom implementation because belief updaters do not typically deal with reward.   #  POMDPModelTools.GenerativeBeliefMDP     Type .  GenerativeBeliefMDP(pomdp, updater)  Create a generative model of the belief MDP corresponding to POMDP  pomdp  with belief updates performed by  updater .  source", 
            "title": "Generative Belief MDP"
        }, 
        {
            "location": "/model_transformations/#example", 
            "text": "using POMDPModels\nusing POMDPModelTools\nusing BeliefUpdaters\n\npomdp = BabyPOMDP()\nupdater = DiscreteUpdater(pomdp)\n\nbelief_mdp = GenerativeBeliefMDP(pomdp, updater)\n@show statetype(belief_mdp) # POMDPModels.BoolDistribution\n\nfor (a, r, sp) in stepthrough(belief_mdp, RandomPolicy(belief_mdp),  a,r,sp , max_steps=5)\n    @show a, r, sp\nend", 
            "title": "Example"
        }, 
        {
            "location": "/model_transformations/#underlying-mdp", 
            "text": "#  POMDPModelTools.UnderlyingMDP     Type .  UnderlyingMDP(pomdp)  Transform  POMDP   pomdp  into an  MDP  where the states are fully observed.  source", 
            "title": "Underlying MDP"
        }, 
        {
            "location": "/utility_types/", 
            "text": "Utility Types\n\n\n\n\nTerminal State\n\n\nTerminalState\n and its singleton instance \nterminalstate\n are available to use for a terminal state in concert with another state type. It has the appropriate type promotion logic to make its use with other types friendly, similar to \nnothing\n and \nmissing\n.\n\n\n NOTE: This is NOT a replacement for the standard POMDPs.jl isterminal function, though isterminal is implemented for the type. It is merely a convenient type to use for terminal states. \n\n\n\n WARNING: Early tests suggest that the Julia 1.0 compiler will not be able to efficiently implement union splitting in cases as complex as POMDPs, so using a `Union` for the state type of a problem can currently have a large overhead. \n\n\n\n#\n\n\nPOMDPModelTools.TerminalState\n \n \nType\n.\n\n\nTerminalState\n\n\n\n\nA type with no fields whose singleton instance \nterminalstate\n is used to represent a terminal state with no additional information.\n\n\nThis type has the appropriate promotion logic implemented to function like \nMissing\n when added to arrays, etc.\n\n\nNote that terminal states NEED NOT be of type \nTerminalState\n. You can define any state to be terminal by implementing the appropriate \nisterminal\n method. Solvers and simulators SHOULD NOT check for this type, but should instead check using \nisterminal\n. \n\n\nsource\n\n\n#\n\n\nPOMDPModelTools.terminalstate\n \n \nConstant\n.\n\n\nterminalstate\n\n\n\n\nThe singleton instance of type \nTerminalState\n representing a terminal state.\n\n\nsource", 
            "title": "Utility types"
        }, 
        {
            "location": "/utility_types/#utility-types", 
            "text": "", 
            "title": "Utility Types"
        }, 
        {
            "location": "/utility_types/#terminal-state", 
            "text": "TerminalState  and its singleton instance  terminalstate  are available to use for a terminal state in concert with another state type. It has the appropriate type promotion logic to make its use with other types friendly, similar to  nothing  and  missing .   NOTE: This is NOT a replacement for the standard POMDPs.jl isterminal function, though isterminal is implemented for the type. It is merely a convenient type to use for terminal states.    WARNING: Early tests suggest that the Julia 1.0 compiler will not be able to efficiently implement union splitting in cases as complex as POMDPs, so using a `Union` for the state type of a problem can currently have a large overhead.   #  POMDPModelTools.TerminalState     Type .  TerminalState  A type with no fields whose singleton instance  terminalstate  is used to represent a terminal state with no additional information.  This type has the appropriate promotion logic implemented to function like  Missing  when added to arrays, etc.  Note that terminal states NEED NOT be of type  TerminalState . You can define any state to be terminal by implementing the appropriate  isterminal  method. Solvers and simulators SHOULD NOT check for this type, but should instead check using  isterminal .   source  #  POMDPModelTools.terminalstate     Constant .  terminalstate  The singleton instance of type  TerminalState  representing a terminal state.  source", 
            "title": "Terminal State"
        }
    ]
}