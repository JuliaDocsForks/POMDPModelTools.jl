{
    "docs": [
        {
            "location": "/", 
            "text": "About\n\n\nPOMDPModelTools is a collection of interface extensions and tools to make writing models and solvers for \nPOMDPs.jl\n easier.\n\n\n\n\nModel Transformations\n\n\nFully Observable POMDP\n\n\nGenerative Belief MDP\n\n\n\n\n\n\nConvenience\n\n\nDistribution Tools\n\n\nWeighted Iteration\n\n\n\n\n\n\nDistributions\n\n\nSparse Categorical (\nSparseCat\n)\n\n\nBool Distribution\n\n\nDeterministic\n\n\n\n\n\n\nAbout", 
            "title": "Home"
        }, 
        {
            "location": "/#about", 
            "text": "POMDPModelTools is a collection of interface extensions and tools to make writing models and solvers for  POMDPs.jl  easier.   Model Transformations  Fully Observable POMDP  Generative Belief MDP    Convenience  Distribution Tools  Weighted Iteration    Distributions  Sparse Categorical ( SparseCat )  Bool Distribution  Deterministic    About", 
            "title": "About"
        }, 
        {
            "location": "/convenience/", 
            "text": "Convenience\n\n\nPOMDPModelTools contains default implementations for some POMDPs.jl functions, including \nactions\n, \nn_actions\n, \naction_index\n, etc. for some obvious cases. This allows some obvious implementations to be skipped.\n\n\nFor instance, if an MDP with a Bool action type is created, the obvious \nactions\n method is already implemented:\n\n\njulia\n using POMDPs; using POMDPModelTools\n\njulia\n struct BoolMDP \n: MDP{Bool, Bool} end\n\njulia\n actions(BoolMDP())\n(true, false)\n\n\n\n\nFor a complete list of default implementations, see \nconvenient_implementations.jl\n.", 
            "title": "Convenience"
        }, 
        {
            "location": "/convenience/#convenience", 
            "text": "POMDPModelTools contains default implementations for some POMDPs.jl functions, including  actions ,  n_actions ,  action_index , etc. for some obvious cases. This allows some obvious implementations to be skipped.  For instance, if an MDP with a Bool action type is created, the obvious  actions  method is already implemented:  julia  using POMDPs; using POMDPModelTools\n\njulia  struct BoolMDP  : MDP{Bool, Bool} end\n\njulia  actions(BoolMDP())\n(true, false)  For a complete list of default implementations, see  convenient_implementations.jl .", 
            "title": "Convenience"
        }, 
        {
            "location": "/distribution_tools/", 
            "text": "Distribution Tools\n\n\nPOMDPModelTools contains an interface extension to make interaction with certain distributions more efficient in some cases.\n\n\n\n\nWeighted Iteration\n\n\n#\n\n\nPOMDPModelTools.weighted_iterator\n \n \nFunction\n.\n\n\nweighted_iterator(d)\n\n\n\n\nReturn an iterator through pairs of the values and probabilities in distribution \nd\n.\n\n\nThis is designed to speed up value iteration. Distributions are encouraged to provide a custom optimized implementation if possible.\n\n\nExample\n\n\njulia\n d = BoolDistribution(0.7)\nBoolDistribution(0.7)\n\njulia\n collect(weighted_iterator(d))\n2-element Array{Pair{Bool,Float64},1}:\n  true =\n 0.7\n false =\n 0.3\n\n\n\n\nsource", 
            "title": "Distribution tools"
        }, 
        {
            "location": "/distribution_tools/#distribution-tools", 
            "text": "POMDPModelTools contains an interface extension to make interaction with certain distributions more efficient in some cases.", 
            "title": "Distribution Tools"
        }, 
        {
            "location": "/distribution_tools/#weighted-iteration", 
            "text": "#  POMDPModelTools.weighted_iterator     Function .  weighted_iterator(d)  Return an iterator through pairs of the values and probabilities in distribution  d .  This is designed to speed up value iteration. Distributions are encouraged to provide a custom optimized implementation if possible.  Example  julia  d = BoolDistribution(0.7)\nBoolDistribution(0.7)\n\njulia  collect(weighted_iterator(d))\n2-element Array{Pair{Bool,Float64},1}:\n  true =  0.7\n false =  0.3  source", 
            "title": "Weighted Iteration"
        }, 
        {
            "location": "/distributions/", 
            "text": "Distributions\n\n\nPOMDPModelTools contains several utility distributions to be used in the POMDPs \ntransition\n and \nobservation\n functions. These implement the appropriate methods of the functions in the \ndistributions interface\n.\n\n\n\n\nSparse Categorical (\nSparseCat\n)\n\n\nSparseCat\n is a sparse categorical distribution which is specified by simply providing a list of possible values (states or observations) and the probabilities corresponding to those particular objects.\n\n\nExample: \nSparseCat([1,2,3], [0.1,0.2,0.7])\n is a categorical distribution that assignes probability 0.1 to \n1\n, 0.2 to \n2\n, 0.7 to \n3\n, and 0 to all other values.\n\n\n#\n\n\nPOMDPModelTools.SparseCat\n \n \nType\n.\n\n\nSparseCat(values, probabilities)\n\n\n\n\nCreate a sparse categorical distribution.\n\n\nvalues\n is an iterable object containing the possible values (can be of any type) in the distribution that have nonzero probability. \nprobabilities\n is an iterable object that contains the associated probabilities.\n\n\nThis is optimized for value iteration with a fast implementation of \nweighted_iterator\n. Both \npdf\n and \nrand\n are order n.\n\n\nsource\n\n\n\n\nBool Distribution\n\n\n#\n\n\nPOMDPModelTools.BoolDistribution\n \n \nType\n.\n\n\nBoolDistribution(p_true)\n\n\n\n\nCreate a distribution over Boolean values (\ntrue\n or \nfalse\n).\n\n\np_true\n is the probability of the \ntrue\n outcome; the probability of \nfalse\n is 1-\np_true\n.\n\n\nsource\n\n\n\n\nDeterministic\n\n\n#\n\n\nPOMDPModelTools.Deterministic\n \n \nType\n.\n\n\nDetrministic(value)\n\n\n\n\nCreate a deterministic distribution over only one value.\n\n\nThis is intended to be used when a distribution is required, but the outcome is deterministic. It is equivalent to a Kronecker Delta distribution.\n\n\nsource", 
            "title": "Distributions"
        }, 
        {
            "location": "/distributions/#distributions", 
            "text": "POMDPModelTools contains several utility distributions to be used in the POMDPs  transition  and  observation  functions. These implement the appropriate methods of the functions in the  distributions interface .", 
            "title": "Distributions"
        }, 
        {
            "location": "/distributions/#sparse-categorical-sparsecat", 
            "text": "SparseCat  is a sparse categorical distribution which is specified by simply providing a list of possible values (states or observations) and the probabilities corresponding to those particular objects.  Example:  SparseCat([1,2,3], [0.1,0.2,0.7])  is a categorical distribution that assignes probability 0.1 to  1 , 0.2 to  2 , 0.7 to  3 , and 0 to all other values.  #  POMDPModelTools.SparseCat     Type .  SparseCat(values, probabilities)  Create a sparse categorical distribution.  values  is an iterable object containing the possible values (can be of any type) in the distribution that have nonzero probability.  probabilities  is an iterable object that contains the associated probabilities.  This is optimized for value iteration with a fast implementation of  weighted_iterator . Both  pdf  and  rand  are order n.  source", 
            "title": "Sparse Categorical (SparseCat)"
        }, 
        {
            "location": "/distributions/#bool-distribution", 
            "text": "#  POMDPModelTools.BoolDistribution     Type .  BoolDistribution(p_true)  Create a distribution over Boolean values ( true  or  false ).  p_true  is the probability of the  true  outcome; the probability of  false  is 1- p_true .  source", 
            "title": "Bool Distribution"
        }, 
        {
            "location": "/distributions/#deterministic", 
            "text": "#  POMDPModelTools.Deterministic     Type .  Detrministic(value)  Create a deterministic distribution over only one value.  This is intended to be used when a distribution is required, but the outcome is deterministic. It is equivalent to a Kronecker Delta distribution.  source", 
            "title": "Deterministic"
        }, 
        {
            "location": "/model_transformations/", 
            "text": "Model Transformations\n\n\nPOMDPModelTools contains several tools for transforming problems into other classes so that they can be used by different solvers.\n\n\n\n\nFully Observable POMDP\n\n\n#\n\n\nPOMDPModelTools.FullyObservablePOMDP\n \n \nType\n.\n\n\nFullyObservablePOMDP(mdp)\n\n\n\n\nTurn \nMDP\n \nmdp\n into a \nPOMDP\n where the observations are the states of the MDP.\n\n\nsource\n\n\n\n\nGenerative Belief MDP\n\n\nEvery POMDP is an MDP on the belief space \nGenerativeBeliefMDP\n creates a generative model for that MDP.\n\n\n The reward generated by the `GenerativeBeliefMDP` is the reward for a *single state sampled from the belief*; it is not the expected reward for that belief transition (though, in expectation, they are equivalent of course). Implementing the model with the expected reward requires a custom implementation because belief updaters do not typically deal with reward. \n\n\n\n#\n\n\nPOMDPModelTools.GenerativeBeliefMDP\n \n \nType\n.\n\n\nGenerativeBeliefMDP(pomdp, updater)\n\n\n\n\nCreate a generative model of the belief MDP corresponding to POMDP \npomdp\n with belief updates performed by \nupdater\n.\n\n\nsource\n\n\n\n\nExample\n\n\nusing POMDPModels\nusing POMDPModelTools\nusing BeliefUpdaters\n\npomdp = BabyPOMDP()\nupdater = DiscreteUpdater(pomdp)\n\nbelief_mdp = GenerativeBeliefMDP(pomdp, updater)\n@show statetype(belief_mdp) # POMDPModels.BoolDistribution\n\nfor (a, r, sp) in stepthrough(belief_mdp, RandomPolicy(belief_mdp), \na,r,sp\n, max_steps=5)\n    @show a, r, sp\nend", 
            "title": "Model transformations"
        }, 
        {
            "location": "/model_transformations/#model-transformations", 
            "text": "POMDPModelTools contains several tools for transforming problems into other classes so that they can be used by different solvers.", 
            "title": "Model Transformations"
        }, 
        {
            "location": "/model_transformations/#fully-observable-pomdp", 
            "text": "#  POMDPModelTools.FullyObservablePOMDP     Type .  FullyObservablePOMDP(mdp)  Turn  MDP   mdp  into a  POMDP  where the observations are the states of the MDP.  source", 
            "title": "Fully Observable POMDP"
        }, 
        {
            "location": "/model_transformations/#generative-belief-mdp", 
            "text": "Every POMDP is an MDP on the belief space  GenerativeBeliefMDP  creates a generative model for that MDP.   The reward generated by the `GenerativeBeliefMDP` is the reward for a *single state sampled from the belief*; it is not the expected reward for that belief transition (though, in expectation, they are equivalent of course). Implementing the model with the expected reward requires a custom implementation because belief updaters do not typically deal with reward.   #  POMDPModelTools.GenerativeBeliefMDP     Type .  GenerativeBeliefMDP(pomdp, updater)  Create a generative model of the belief MDP corresponding to POMDP  pomdp  with belief updates performed by  updater .  source", 
            "title": "Generative Belief MDP"
        }, 
        {
            "location": "/model_transformations/#example", 
            "text": "using POMDPModels\nusing POMDPModelTools\nusing BeliefUpdaters\n\npomdp = BabyPOMDP()\nupdater = DiscreteUpdater(pomdp)\n\nbelief_mdp = GenerativeBeliefMDP(pomdp, updater)\n@show statetype(belief_mdp) # POMDPModels.BoolDistribution\n\nfor (a, r, sp) in stepthrough(belief_mdp, RandomPolicy(belief_mdp),  a,r,sp , max_steps=5)\n    @show a, r, sp\nend", 
            "title": "Example"
        }
    ]
}